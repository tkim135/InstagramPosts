{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = os.path.join('../tkim6/riri145/post-data.csv')\n",
    "df = pd.read_csv(csvfile, skiprows=21404, \n",
    "                 usecols=[0, 1, 2, 3, 4, 5, 6, 7],\n",
    "                names=['id', 'date',  'caption', 'image_name', 'tagged_users', 'likes', 'comments', 'ad'])\n",
    "def cleanup(caption):\n",
    "    return caption.replace('#ad', '').replace('#sponsored', '').replace('#Ad', '').replace('#advertisement', '')\n",
    "df['caption'] = df['caption'].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.caption\n",
    "Y = df.ad\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1, 1)\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "sequences_matrix = np.load('sequences_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    tok = Tokenizer(num_words=max_words)\n",
    "    tok.fit_on_texts(X)\n",
    "    sequences = tok.texts_to_sequences(X)\n",
    "    sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "    np.save('sequences_matrix.npy', sequences_matrix)\n",
    "else:\n",
    "    print(sequences_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-50 and replace top fc layer.\"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "#         self.fc1 = nn.Linear(2048, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        features = self.avgpool(features)\n",
    "        features = features.view(features.size(0), -1)  # reshape\n",
    "        #return features\n",
    "#         features = F.relu(self.fc1(features))\n",
    "#         features = F.relu(self.fc2(features))\n",
    "#         features = self.fc3(features)\n",
    "#         features = torch.sigmoid(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN1():\n",
    "    model = nn.Sequential(\n",
    "        nn.Embedding(max_words, 64),\n",
    "        nn.LSTM(64, max_words))\n",
    "    return model\n",
    "\n",
    "def RNN2():\n",
    "    model = nn.Sequential(\n",
    "        nn.ReLU(inplace=False),\n",
    "        nn.Dropout(0.5),\n",
    "#           nn.Dense(1),\n",
    "        nn.Linear(150000, 2048),\n",
    "#         nn.Linear(2000, 100),\n",
    "#         nn.Linear(100,2),\n",
    "        #nn.MaxPool1d(2, 1000),\n",
    "        #nn.MaxPool2d(1, 75),\n",
    "        nn.Sigmoid())\n",
    "        \n",
    "    return model\n",
    "        \n",
    "def LinearGravityBong():\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(4096, 256),\n",
    "        nn.Linear(256, 16),\n",
    "        nn.Linear(16, 2),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA_PATH = '/home/tkim6/riri145/img/'\n",
    "DEFAULT_SAVED_LABELS = '/home/tkim6/riri145/preloaded.pt'\n",
    "\n",
    "class InstagramDataset(Dataset):\n",
    "    '''\n",
    "    Characterizes a dataset for PyTorch.\n",
    "    '''\n",
    "    def __init__(self, dataset_path=DEFAULT_DATA_PATH, \n",
    "                label_path = DEFAULT_SAVED_LABELS, transform=None):\n",
    "\n",
    "        # Checks if pre-saved training labels are available.\n",
    "        # If not, loads from csv file and saves to a .pt file\n",
    "        # (pytorch default save extension) to be loaded up in the future.\n",
    "        if not os.path.exists(label_path):\n",
    "\n",
    "\n",
    "            # Creates dictionary to save all image names and labels\n",
    "            \n",
    "            def oneHot(label):\n",
    "                if label == 1:\n",
    "                    return torch.tensor([0, 1], dtype=torch.float32)\n",
    "                else:\n",
    "                    return torch.tensor([1, 0], dtype=torch.float32)\n",
    "            new_labels = [oneHot(label) for label in df['ad'].values.tolist()]\n",
    "            \n",
    "            data_dict = {\n",
    "                'image_names': df['image_name'].values.tolist(),\n",
    "                'caption': df['caption'].values.tolist(),\n",
    "                'labels': new_labels,\n",
    "            }\n",
    "            \n",
    "            \n",
    "\n",
    "            # Iterates through csv file and grabs image names + labels\n",
    "            '''for idx, line in enumerate(csv_reader):\n",
    "                if idx > 21404 and ('jpg' in line[3] or 'png' in line[3]):\n",
    "                    data_dict['image_names'].append(line[3])\n",
    "                    data_dict['caption'].append(line[2])\n",
    "                    if int(line[-1]) == 1:\n",
    "                        data_dict['labels'].append(torch.tensor([1, 0], dtype=torch.float32))\n",
    "                    else:\n",
    "                        data_dict['labels'].append(torch.tensor([0, 1], dtype=torch.float32))'''\n",
    "\n",
    "            # Saves for easy loading next time\n",
    "            if not os.path.isdir(label_path[:label_path.rfind('/')]):\n",
    "                os.makedirs(label_path[:label_path.rfind('/')])\n",
    "            torch.save(data_dict, label_path)\n",
    "\n",
    "        # Otherwise, just load the pre-saved dict.\n",
    "        else:\n",
    "            data_dict = torch.load(label_path)\n",
    "\n",
    "        # Saves state variables\n",
    "        self.data_dict = data_dict\n",
    "        self.dataset_path = dataset_path\n",
    "        self.label_path = label_path\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the total number of samples'''\n",
    "        return len(self.data_dict['labels'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Generates one sample of data'''\n",
    "        # Select sample\n",
    "        image_name = self.data_dict['image_names'][index]\n",
    "        \n",
    "        X = torch.zeros(3, 224, 224)\n",
    "        try:\n",
    "            X = self.transform(Image.open(self.dataset_path + image_name))\n",
    "        except:\n",
    "            X = torch.zeros(3, 224, 224)\n",
    "        y = self.data_dict['labels'][index]\n",
    "\n",
    "        return X, np.asarray(sequences_matrix[index], dtype=int), y\n",
    "\n",
    "\n",
    "def get_dataloaders(dataset_path=DEFAULT_DATA_PATH, \n",
    "                    label_path = DEFAULT_SAVED_LABELS, val_split=0.2, batch_sz=1,\n",
    "                    num_threads=1, shuffle_val=True):\n",
    "    '''\n",
    "    Grabs dataloaders for train/val sets.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    > dataset_path (string) -- Path to folder where all dataset images are stored.\n",
    "    > label_path (string) -- Path to saved labels (should be .pt file).\n",
    "    > val_split (float) -- Fraction of training data to be used as validation set.\n",
    "    > batch_sz (int) -- Batch size to be grabbed from DataLoader.\n",
    "    > num_threads (int) -- Number of threads with which to load data.\n",
    "    > shuffle_val (bool) -- Whether to shuffle validation set indices.\n",
    "\n",
    "    Return value: (train_dataloader, test_dataloader)\n",
    "    > train_dataloader -- a torch.utils.data.DataLoader wrapper around\n",
    "        the specified dataset's training set.\n",
    "    > val_dataloader -- a torch.utils.data.DataLoader wrapper around\n",
    "        the specified dataset's validation set.\n",
    "    '''\n",
    "\n",
    "    # Describes the transforms we want. Using randomCrop and toTensor.\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), # 128 x 128 random crop of image.\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    # Constructs InstagramDataset to load data from\n",
    "    dataset = InstagramDataset(dataset_path=dataset_path,\n",
    "                                label_path=label_path, transform=transform)\n",
    "\n",
    "    # Grabs train/val split\n",
    "    num_train = len(dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(val_split * num_train))\n",
    "\n",
    "    # Shuffle indices if ncessary for slicing val set\n",
    "    if shuffle_val:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    # Performs train/val split\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # Constructs dataloader wrappers around InstagramDataset training and test sets\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_sz, \n",
    "                                  num_workers=num_threads, sampler=train_sampler)\n",
    "    val_dataloader = DataLoader(dataset, batch_size=batch_sz, \n",
    "                                num_workers=num_threads, sampler=val_sampler)\n",
    "\n",
    "    return (train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def print(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "\n",
    "        _, targets = target.topk(maxk, 1, True, True)\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res, pred.numpy(), targets.t().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network1, network2, network69, linear_gravity_bong, train_loader, val_loader):\n",
    "    # THIS IS BAD. PLS REFACTOR\n",
    "    #print(network.forward)\n",
    "    learning_rate = 1e-2\n",
    "    momentum = 0.9\n",
    "    optimizer = torch.optim.SGD(network1.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    criterion = nn.BCELoss()\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_time = AverageMeter('Time', ':6.3f')\n",
    "        data_time = AverageMeter('Data', ':6.3f')\n",
    "        losses = AverageMeter('Loss', ':.4e')\n",
    "        top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "        progress = ProgressMeter(len(train_loader), batch_time, data_time, losses, top1,\n",
    "                                prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        end = time.time()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #print(i, data)\n",
    "            input, caption, target = data\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            '''# No GPU yet\n",
    "            args = get_args()\n",
    "            if args.gpu is not None:\n",
    "                input = input.cuda(args.gpu, non_blocking=True)\n",
    "            target = target.cuda(args.gpu, non_blocking=True)'''\n",
    "            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "            input.to(device)\n",
    "            caption.to(device)\n",
    "            target.to(device)\n",
    "\n",
    "            output, (h, c) = network1(caption)\n",
    "            output = output.reshape(1, max_len * max_words)\n",
    "            output = network2(output)\n",
    "            ensemble = torch.cat((output, network69(input)), dim=1)\n",
    "            \n",
    "            output = linear_gravity_bong(ensemble)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1 = accuracy(output, target)[0][0]\n",
    "            losses.update(loss.item(), caption.size(0))\n",
    "            top1.update(acc1[0], caption.size(0))\n",
    "            #losses.update(loss.item(), input.size(0))\n",
    "            #top1.update(acc1[0], input.size(0))\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                progress.print(i)\n",
    "\n",
    "            # Save weights\n",
    "            if i % 100 == 99:\n",
    "                print('outputs:', output)\n",
    "                # validate(val_loader, network, criterion)\n",
    "                if not os.path.isdir('weights/'):\n",
    "                    os.makedirs('weights/')\n",
    "                torch.save({\n",
    "                        'network1': network1,\n",
    "                        'network2': network2,\n",
    "                        'network69': network69,\n",
    "                        'linear_gravity_bong': linear_gravity_bong,\n",
    "                        'optimizer': optimizer,\n",
    "                        'criterion': criterion,\n",
    "                    },\n",
    "                    'weights/weights_epoch_' + str(epoch) + '_iteration_' + str(i).zfill(6) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][     0/159704]\tTime  6.454 ( 6.454)\tData  0.130 ( 0.130)\tLoss 7.6574e-01 (7.6574e-01)\tAcc@1   0.00 (  0.00)\n",
      "Epoch: [0][    20/159704]\tTime  2.535 ( 2.730)\tData  0.003 ( 0.009)\tLoss 7.6766e-01 (7.1237e-01)\tAcc@1   0.00 ( 38.10)\n",
      "Epoch: [0][    40/159704]\tTime  2.528 ( 2.636)\tData  0.003 ( 0.006)\tLoss 6.2410e-01 (7.0769e-01)\tAcc@1 100.00 ( 41.46)\n",
      "Epoch: [0][    60/159704]\tTime  2.539 ( 2.604)\tData  0.003 ( 0.005)\tLoss 7.6609e-01 (7.0612e-01)\tAcc@1   0.00 ( 42.62)\n",
      "Epoch: [0][    80/159704]\tTime  2.525 ( 2.588)\tData  0.003 ( 0.005)\tLoss 7.6693e-01 (7.0187e-01)\tAcc@1   0.00 ( 45.68)\n",
      "outputs: tensor([[0.4622, 0.5340]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   100/159704]\tTime 11.691 ( 2.671)\tData  9.184 ( 0.095)\tLoss 7.6709e-01 (7.0344e-01)\tAcc@1   0.00 ( 44.55)\n",
      "Epoch: [0][   120/159704]\tTime  2.517 ( 2.651)\tData  0.003 ( 0.080)\tLoss 6.2179e-01 (6.9975e-01)\tAcc@1 100.00 ( 47.11)\n",
      "Epoch: [0][   140/159704]\tTime  2.577 ( 2.636)\tData  0.003 ( 0.069)\tLoss 7.6570e-01 (6.9612e-01)\tAcc@1   0.00 ( 49.65)\n",
      "Epoch: [0][   160/159704]\tTime  2.582 ( 2.628)\tData  0.003 ( 0.061)\tLoss 7.6713e-01 (6.9692e-01)\tAcc@1   0.00 ( 49.07)\n",
      "Epoch: [0][   180/159704]\tTime  2.534 ( 2.619)\tData  0.003 ( 0.054)\tLoss 6.2350e-01 (6.9523e-01)\tAcc@1 100.00 ( 50.28)\n",
      "outputs: tensor([[0.4604, 0.5322]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][   200/159704]\tTime 12.388 ( 2.661)\tData  9.843 ( 0.098)\tLoss 6.2294e-01 (6.9600e-01)\tAcc@1 100.00 ( 49.75)\n",
      "Epoch: [0][   220/159704]\tTime  2.541 ( 2.652)\tData  0.003 ( 0.090)\tLoss 6.2431e-01 (6.9531e-01)\tAcc@1 100.00 ( 50.23)\n",
      "Epoch: [0][   240/159704]\tTime  2.536 ( 2.644)\tData  0.005 ( 0.082)\tLoss 7.6671e-01 (6.9594e-01)\tAcc@1   0.00 ( 49.79)\n",
      "Epoch: [0][   260/159704]\tTime  2.539 ( 2.637)\tData  0.003 ( 0.076)\tLoss 7.6555e-01 (6.9320e-01)\tAcc@1   0.00 ( 51.72)\n",
      "Epoch: [0][   280/159704]\tTime  2.547 ( 2.629)\tData  0.003 ( 0.071)\tLoss 7.6853e-01 (6.9287e-01)\tAcc@1   0.00 ( 51.96)\n",
      "outputs: tensor([[0.4623, 0.5335]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][   300/159704]\tTime 11.742 ( 2.655)\tData  9.147 ( 0.097)\tLoss 7.6735e-01 (6.9261e-01)\tAcc@1   0.00 ( 52.16)\n",
      "Epoch: [0][   320/159704]\tTime  2.577 ( 2.651)\tData  0.003 ( 0.091)\tLoss 6.2489e-01 (6.9325e-01)\tAcc@1 100.00 ( 51.71)\n",
      "Epoch: [0][   340/159704]\tTime  2.570 ( 2.646)\tData  0.003 ( 0.086)\tLoss 6.2688e-01 (6.9504e-01)\tAcc@1 100.00 ( 50.44)\n",
      "Epoch: [0][   360/159704]\tTime  2.572 ( 2.642)\tData  0.003 ( 0.081)\tLoss 6.2189e-01 (6.9549e-01)\tAcc@1 100.00 ( 50.14)\n",
      "Epoch: [0][   380/159704]\tTime  2.575 ( 2.638)\tData  0.003 ( 0.077)\tLoss 6.2407e-01 (6.9546e-01)\tAcc@1 100.00 ( 50.13)\n",
      "outputs: tensor([[0.4626, 0.5337]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][   400/159704]\tTime 11.642 ( 2.657)\tData  9.080 ( 0.096)\tLoss 7.6724e-01 (6.9548e-01)\tAcc@1   0.00 ( 50.12)\n",
      "Epoch: [0][   420/159704]\tTime  2.550 ( 2.653)\tData  0.003 ( 0.092)\tLoss 6.2280e-01 (6.9584e-01)\tAcc@1 100.00 ( 49.88)\n",
      "Epoch: [0][   440/159704]\tTime  2.542 ( 2.648)\tData  0.003 ( 0.088)\tLoss 6.2373e-01 (6.9582e-01)\tAcc@1 100.00 ( 49.89)\n",
      "Epoch: [0][   460/159704]\tTime  2.557 ( 2.644)\tData  0.003 ( 0.084)\tLoss 7.6740e-01 (6.9583e-01)\tAcc@1   0.00 ( 49.89)\n",
      "Epoch: [0][   480/159704]\tTime  2.548 ( 2.640)\tData  0.003 ( 0.081)\tLoss 6.2421e-01 (6.9552e-01)\tAcc@1 100.00 ( 50.10)\n",
      "outputs: tensor([[0.4622, 0.5339]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][   500/159704]\tTime 11.881 ( 2.655)\tData  9.369 ( 0.096)\tLoss 6.2536e-01 (6.9553e-01)\tAcc@1 100.00 ( 50.10)\n",
      "Epoch: [0][   520/159704]\tTime  2.529 ( 2.651)\tData  0.003 ( 0.093)\tLoss 7.6590e-01 (6.9610e-01)\tAcc@1   0.00 ( 49.71)\n",
      "Epoch: [0][   540/159704]\tTime  2.543 ( 2.647)\tData  0.003 ( 0.089)\tLoss 7.6892e-01 (6.9527e-01)\tAcc@1   0.00 ( 50.28)\n",
      "Epoch: [0][   560/159704]\tTime  2.548 ( 2.643)\tData  0.003 ( 0.086)\tLoss 6.2297e-01 (6.9580e-01)\tAcc@1 100.00 ( 49.91)\n",
      "Epoch: [0][   580/159704]\tTime  2.554 ( 2.640)\tData  0.003 ( 0.083)\tLoss 7.6550e-01 (6.9627e-01)\tAcc@1   0.00 ( 49.57)\n",
      "Epoch: [0][   640/159704]\tTime  2.582 ( 2.646)\tData  0.004 ( 0.090)\tLoss 6.2556e-01 (6.9534e-01)\tAcc@1 100.00 ( 50.23)\n",
      "Epoch: [0][   660/159704]\tTime  2.553 ( 2.644)\tData  0.003 ( 0.087)\tLoss 6.2499e-01 (6.9578e-01)\tAcc@1 100.00 ( 49.92)\n",
      "Epoch: [0][   680/159704]\tTime  2.560 ( 2.641)\tData  0.003 ( 0.085)\tLoss 7.6730e-01 (6.9579e-01)\tAcc@1   0.00 ( 49.93)\n",
      "outputs: tensor([[0.4624, 0.5348]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][   700/159704]\tTime 11.585 ( 2.652)\tData  9.041 ( 0.095)\tLoss 6.2096e-01 (6.9600e-01)\tAcc@1 100.00 ( 49.79)\n",
      "Epoch: [0][   720/159704]\tTime  2.542 ( 2.649)\tData  0.003 ( 0.093)\tLoss 6.2341e-01 (6.9659e-01)\tAcc@1 100.00 ( 49.38)\n",
      "Epoch: [0][   740/159704]\tTime  2.549 ( 2.646)\tData  0.003 ( 0.090)\tLoss 6.2584e-01 (6.9638e-01)\tAcc@1 100.00 ( 49.53)\n",
      "Epoch: [0][   760/159704]\tTime  2.586 ( 2.644)\tData  0.004 ( 0.088)\tLoss 7.6717e-01 (6.9656e-01)\tAcc@1   0.00 ( 49.41)\n",
      "Epoch: [0][   780/159704]\tTime  2.547 ( 2.641)\tData  0.003 ( 0.086)\tLoss 6.2600e-01 (6.9636e-01)\tAcc@1 100.00 ( 49.55)\n",
      "outputs: tensor([[0.4603, 0.5321]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][   800/159704]\tTime 11.564 ( 2.650)\tData  9.009 ( 0.095)\tLoss 7.6797e-01 (6.9687e-01)\tAcc@1   0.00 ( 49.19)\n",
      "Epoch: [0][   820/159704]\tTime  2.560 ( 2.648)\tData  0.003 ( 0.093)\tLoss 6.2327e-01 (6.9685e-01)\tAcc@1 100.00 ( 49.21)\n",
      "Epoch: [0][   840/159704]\tTime  2.572 ( 2.646)\tData  0.003 ( 0.091)\tLoss 7.6846e-01 (6.9699e-01)\tAcc@1   0.00 ( 49.11)\n",
      "Epoch: [0][   860/159704]\tTime  2.555 ( 2.644)\tData  0.003 ( 0.089)\tLoss 7.6390e-01 (6.9761e-01)\tAcc@1   0.00 ( 48.66)\n",
      "Epoch: [0][   880/159704]\tTime  2.564 ( 2.642)\tData  0.003 ( 0.087)\tLoss 7.6741e-01 (6.9740e-01)\tAcc@1   0.00 ( 48.81)\n",
      "outputs: tensor([[0.4605, 0.5344]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][   900/159704]\tTime 11.730 ( 2.651)\tData  9.072 ( 0.095)\tLoss 7.6727e-01 (6.9753e-01)\tAcc@1   0.00 ( 48.72)\n",
      "Epoch: [0][   920/159704]\tTime  2.586 ( 2.649)\tData  0.003 ( 0.093)\tLoss 6.2592e-01 (6.9781e-01)\tAcc@1 100.00 ( 48.53)\n",
      "Epoch: [0][   940/159704]\tTime  2.557 ( 2.647)\tData  0.003 ( 0.091)\tLoss 7.6585e-01 (6.9822e-01)\tAcc@1   0.00 ( 48.25)\n",
      "Epoch: [0][   960/159704]\tTime  2.578 ( 2.646)\tData  0.003 ( 0.089)\tLoss 7.6754e-01 (6.9846e-01)\tAcc@1   0.00 ( 48.07)\n",
      "Epoch: [0][   980/159704]\tTime  2.577 ( 2.645)\tData  0.003 ( 0.087)\tLoss 7.6684e-01 (6.9868e-01)\tAcc@1   0.00 ( 47.91)\n",
      "outputs: tensor([[0.4607, 0.5323]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][  1000/159704]\tTime  6.314 ( 2.647)\tData  3.700 ( 0.089)\tLoss 7.6301e-01 (6.9848e-01)\tAcc@1   0.00 ( 48.05)\n",
      "Epoch: [0][  1020/159704]\tTime  2.554 ( 2.645)\tData  0.003 ( 0.088)\tLoss 6.2484e-01 (6.9871e-01)\tAcc@1 100.00 ( 47.89)\n",
      "Epoch: [0][  1040/159704]\tTime  2.564 ( 2.643)\tData  0.003 ( 0.086)\tLoss 6.2548e-01 (6.9852e-01)\tAcc@1 100.00 ( 48.03)\n",
      "Epoch: [0][  1060/159704]\tTime  2.556 ( 2.642)\tData  0.003 ( 0.085)\tLoss 7.6733e-01 (6.9873e-01)\tAcc@1   0.00 ( 47.88)\n",
      "Epoch: [0][  1080/159704]\tTime  2.564 ( 2.640)\tData  0.003 ( 0.083)\tLoss 7.6657e-01 (6.9907e-01)\tAcc@1   0.00 ( 47.64)\n",
      "outputs: tensor([[0.4613, 0.5326]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][  1100/159704]\tTime  7.405 ( 2.643)\tData  4.818 ( 0.086)\tLoss 7.6559e-01 (6.9939e-01)\tAcc@1   0.00 ( 47.41)\n",
      "Epoch: [0][  1120/159704]\tTime  2.545 ( 2.642)\tData  0.003 ( 0.085)\tLoss 6.2418e-01 (6.9932e-01)\tAcc@1 100.00 ( 47.46)\n",
      "Epoch: [0][  1140/159704]\tTime  2.557 ( 2.640)\tData  0.003 ( 0.083)\tLoss 7.6614e-01 (6.9926e-01)\tAcc@1   0.00 ( 47.50)\n",
      "Epoch: [0][  1160/159704]\tTime  2.547 ( 2.638)\tData  0.003 ( 0.082)\tLoss 7.6501e-01 (6.9944e-01)\tAcc@1   0.00 ( 47.37)\n",
      "Epoch: [0][  1180/159704]\tTime  2.566 ( 2.637)\tData  0.003 ( 0.080)\tLoss 7.6724e-01 (6.9950e-01)\tAcc@1   0.00 ( 47.33)\n",
      "outputs: tensor([[0.4604, 0.5331]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][  1200/159704]\tTime  7.579 ( 2.640)\tData  4.964 ( 0.083)\tLoss 6.2831e-01 (6.9933e-01)\tAcc@1 100.00 ( 47.46)\n",
      "Epoch: [0][  1220/159704]\tTime  2.577 ( 2.639)\tData  0.003 ( 0.082)\tLoss 6.2213e-01 (6.9949e-01)\tAcc@1 100.00 ( 47.34)\n",
      "Epoch: [0][  1240/159704]\tTime  2.566 ( 2.637)\tData  0.003 ( 0.081)\tLoss 6.2463e-01 (6.9932e-01)\tAcc@1 100.00 ( 47.46)\n",
      "Epoch: [0][  1260/159704]\tTime  2.550 ( 2.636)\tData  0.003 ( 0.079)\tLoss 7.6363e-01 (6.9960e-01)\tAcc@1   0.00 ( 47.26)\n",
      "Epoch: [0][  1280/159704]\tTime  2.556 ( 2.635)\tData  0.003 ( 0.078)\tLoss 6.2317e-01 (6.9942e-01)\tAcc@1 100.00 ( 47.38)\n",
      "outputs: tensor([[0.4618, 0.5328]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][  1300/159704]\tTime  6.346 ( 2.637)\tData  3.748 ( 0.080)\tLoss 6.2389e-01 (6.9958e-01)\tAcc@1 100.00 ( 47.27)\n",
      "Epoch: [0][  1320/159704]\tTime  2.558 ( 2.636)\tData  0.003 ( 0.079)\tLoss 6.2376e-01 (6.9984e-01)\tAcc@1 100.00 ( 47.09)\n",
      "Epoch: [0][  1340/159704]\tTime  2.570 ( 2.635)\tData  0.003 ( 0.078)\tLoss 7.6668e-01 (6.9978e-01)\tAcc@1   0.00 ( 47.13)\n",
      "Epoch: [0][  1360/159704]\tTime  2.573 ( 2.633)\tData  0.003 ( 0.077)\tLoss 6.2580e-01 (6.9962e-01)\tAcc@1 100.00 ( 47.24)\n",
      "Epoch: [0][  1380/159704]\tTime  2.539 ( 2.632)\tData  0.003 ( 0.075)\tLoss 6.2435e-01 (6.9998e-01)\tAcc@1 100.00 ( 46.99)\n",
      "Epoch: [0][  1400/159704]\tTime  6.911 ( 2.635)\tData  4.372 ( 0.078)\tLoss 7.6498e-01 (6.9982e-01)\tAcc@1   0.00 ( 47.11)\n",
      "Epoch: [0][  1420/159704]\tTime  2.591 ( 2.633)\tData  0.003 ( 0.076)\tLoss 7.6671e-01 (6.9986e-01)\tAcc@1   0.00 ( 47.08)\n",
      "Epoch: [0][  1440/159704]\tTime  2.555 ( 2.632)\tData  0.003 ( 0.075)\tLoss 7.6647e-01 (7.0009e-01)\tAcc@1   0.00 ( 46.91)\n",
      "Epoch: [0][  1460/159704]\tTime  2.554 ( 2.631)\tData  0.003 ( 0.074)\tLoss 6.2176e-01 (7.0002e-01)\tAcc@1 100.00 ( 46.95)\n",
      "Epoch: [0][  1480/159704]\tTime  2.548 ( 2.630)\tData  0.003 ( 0.074)\tLoss 7.6670e-01 (7.0005e-01)\tAcc@1   0.00 ( 46.93)\n",
      "outputs: tensor([[0.4626, 0.5330]], grad_fn=<SigmoidBackward>)\n",
      "Epoch: [0][  1500/159704]\tTime  7.145 ( 2.632)\tData  4.511 ( 0.076)\tLoss 7.6561e-01 (7.0019e-01)\tAcc@1   0.00 ( 46.84)\n",
      "Epoch: [0][  1520/159704]\tTime  2.581 ( 2.631)\tData  0.003 ( 0.075)\tLoss 7.6708e-01 (7.0023e-01)\tAcc@1   0.00 ( 46.81)\n",
      "Epoch: [0][  1540/159704]\tTime  2.557 ( 2.631)\tData  0.003 ( 0.074)\tLoss 7.6803e-01 (6.9980e-01)\tAcc@1   0.00 ( 47.11)\n",
      "Epoch: [0][  1560/159704]\tTime  2.576 ( 2.630)\tData  0.003 ( 0.073)\tLoss 7.6463e-01 (7.0011e-01)\tAcc@1   0.00 ( 46.89)\n",
      "Epoch: [0][  1580/159704]\tTime  2.559 ( 2.629)\tData  0.003 ( 0.072)\tLoss 7.6583e-01 (6.9969e-01)\tAcc@1   0.00 ( 47.19)\n"
     ]
    }
   ],
   "source": [
    "#model = MockupModel()\n",
    "#model = RNN()\n",
    "model1 = RNN1()\n",
    "model2 = RNN2()\n",
    "model69 = Net()\n",
    "linear_gravity_bong = LinearGravityBong()\n",
    "train_loader, val_loader = get_dataloaders()\n",
    "model = train(model1, model2, model69, linear_gravity_bong, train_loader, val_loader)\n",
    "#model = train(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(Y_test['caption'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen=max_len)\n",
    "pred = model.predict(test_sequences_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
